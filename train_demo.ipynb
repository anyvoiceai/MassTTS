{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7975e2e5",
   "metadata": {},
   "source": [
    "# åˆ©ç”¨Modelscopeçš„æ–‡ä»¶å¤„ç†å’ŒNeMoè®­ç»ƒfs2\n",
    "Use modelscope for data processing & use nemo for tts training.\n",
    "\n",
    "æµç¨‹:\n",
    "1. æ•°æ®å¤„ç† \n",
    "3. è·å–è¾…åŠ©ç‰¹å¾ \n",
    "4. è½¬ä¸ºNeMoæ‰€éœ€æ ¼å¼,ä»¥åŠå…¶ä»–æ‚é¡¹ \n",
    "\n",
    "\n",
    "æœ¬æ¬¡é‡‡ç”¨äº†å³°å“¥ç´ æåº“ 2022å¹´æ‰€æœ‰ç›´æ’­,æœ€åä¿ç•™çº¦40æ¬¡ç›´æ’­, çº¦50å°æ—¶.\n",
    "\n",
    "**å®é™…ä¸Šå¦‚æœä¸ºäº†ä¸è®¡åˆ’å­¦ä¹ ä¸€ä¸ªå¤æ‚çš„æƒ…æ„Ÿ(å³°å“¥çš„æƒ…æ„Ÿå¤§å¤šæ•°æ—¶å€™ä¸ç®—å¤æ‚hh), 10ä¸ªå°æ—¶å·¦å³åŸºæœ¬å¯ä»¥æ»¡è¶³è¦æ±‚.ä¸‹è¿°æ—¶é—´éƒ½æ˜¯åœ¨10hæ•°æ®ä¸Šå®Œæˆçš„.**\n",
    "\n",
    "**æ‰€æœ‰å®éªŒåœ¨3090å’ŒåŒç­‰ç®—åŠ›çš„æ˜¾å¡ä¸Šå®Œæˆ,æ˜¾å­˜è¦æ±‚å‡å°äº24g. å¦‚æœä½ æœ‰3090å’Œä¸€ä¸ª24æ ¸çš„CPU,ä¸‹è¿°çš„æ—¶é—´éƒ½æ˜¯å‡†ç¡®çš„.**\n",
    "\n",
    "æœ€å,åˆ¶ä½œè§†é¢‘å’Œnotebookéƒ½èŠ±è´¹äº†ä¸å°‘æ—¶é—´å’Œç²¾åŠ›,å¸Œæœ›å¤§å®¶å¤šå¤šå…³æ³¨, å¤šå¤šä¸‰è¿.\n",
    "<!-- \n",
    "Steps:\n",
    "1. data processing\n",
    "2. get auxiliary features\n",
    "3. trans metadata to nemo format, run fastpitch and hifigan -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f368f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# pip install \"modelscope[audio]\" -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2628c",
   "metadata": {},
   "source": [
    "----------\n",
    "## æ•°æ®å¤„ç†pipeline\n",
    "\n",
    "æ›´æ”¹äº†modelscopeçš„å‡ ä¸ªé—®é¢˜(æ„Ÿå…´è¶£å¯ä»¥å¸®å¿™mergeä¸€ä¸‹):\n",
    "\n",
    "1. é‡è¦! funasr/bin/vad_inference.py æŠ¥é”™ (æ›´æ–°åˆ°modelscope1.4.3åæŠ¥é”™,ä¹‹å‰çš„ç‰ˆæœ¬å¥½åƒä¸ä¼š)\n",
    "```\n",
    "funasr/bin/vad_inference.py line 275:\n",
    "    åˆ é™¤ results[i] = json.loads(results[i])\n",
    "```\n",
    "\n",
    "1. funasr/modules/nets_utils.py pad_liståœ¨len(xs) == 1æ—¶ä¸å†pad *(å‡å°CPUå ç”¨)*\n",
    "```\n",
    "funasr/modules/nets_utils.py line 54:\n",
    "    if len(xs) == 1:\n",
    "        return xs\n",
    "```\n",
    "\n",
    "2. funasr/models/frontend/wav_frontend.pyåœ¨ apply_cmvn æ—¶ä½¿ç”¨to(device)æ”¹ä¸ºtorch.tensor(, device=device) *(å‡å°CPUå ç”¨)*\n",
    "```\n",
    "funasr/models/frontend/wav_frontend.py line 53:\n",
    "    inputs += torch.tensor(means, dtype=dtype, device=device)\n",
    "    inputs *= torch.tensor(vars, dtype=dtype, device=device)\n",
    "```\n",
    "\n",
    "funasr/utils/postprocess_utils.py @çš„é—®é¢˜,åœ¨batch inferæ—¶å‡ºç°çš„é”™è¯¯. æ„Ÿè°¢modelscopeé’‰é’‰ç¾¤çš„è§£ç­”!\n",
    "```\n",
    "funasr/utils/postprocess_utils.py line 9:\n",
    "    åˆ é™¤åˆ†æ”¯ ch == '@'\n",
    "```\n",
    "\n",
    "funasr/bin/asr_inference_paraformer.py self.frontend.forward() *(æœªä½¿ç”¨GPUåŠ é€Ÿ)*\n",
    "```\n",
    "funasr/bin/asr_inference_paraformer.py line 213:\n",
    "    feats, feats_len = self.frontend.forward(speech.to(self.device), speech_lengths.to(self.device))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ce161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import soundfile as sf\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import IPython.display as iply\n",
    "\n",
    "from pqdm.processes import pqdm\n",
    "from pqdm.threads import pqdm as pqdmT\n",
    "import pandas as pd\n",
    "\n",
    "import tgt\n",
    "from pypinyin import lazy_pinyin, Style\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81472b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸€ä¸‹è·¯å¾„, åœ¨è¯¥è·¯å¾„ä¸‹,æŠŠéŸ³é¢‘æ”¾å…¥/raw/\n",
    "VERSION = ''\n",
    "start_path = f\"./{VERSION}\"\n",
    "if start_path[-1] != '/':\n",
    "    start_path = start_path + '/'\n",
    "    \n",
    "os.makedirs(os.path.join(start_path, 'raw'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiprocess(x):\n",
    "    DEVICE, IJOB = x\n",
    "    subprocess.run(f\"CUDA_VISIBLE_DEVICES={DEVICE} \"+\n",
    "                   f\"nohup python {os.path.join(start_path, 'tmp', 'temp.py')} {IJOB} \"+\n",
    "                   f\"> {os.path.join(start_path, 'tmp','tmp_'+str(IJOB))} 2>&1\",\n",
    "                   shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10737f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050\n",
    "sg_len = 300 # ç§’, 5åˆ†é’Ÿ*60\n",
    "CPU_kernels = 64 # å»ºè®®æœ‰å‡ æ ¸å°±å†™å‡ æ ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befe26e",
   "metadata": {},
   "source": [
    "### æ•°æ®å®šä¹‰å’Œåˆ‡ç‰‡ (CPUå‹å¥½ä»»åŠ¡,é€Ÿåº¦åº”è¯¥å¾ˆå¿«)\n",
    "\n",
    "1. é¢„ç­›é€‰äº†ä¸€äº›æ²¡æœ‰è¿éº¦,éŸ³ä¹è¾ƒå°‘çš„è§†é¢‘,ä¿ç•™å¹¶åˆ é™¤å…¶ä»–çš„å¥‡æ€ªè§†é¢‘(æ¯”å¦‚æˆ·å¤–).\n",
    "2. æ•°æ®åˆ‡ç‰‡, å°†æ•°æ®è½¬æ¢æˆ16000é‡‡æ ·ç‡(å»ºè®®22050,å› ä¸ºæœ‰trainå¥½çš„hifigan), wavæ ¼å¼, ä¸”æ¯æ®µ5åˆ†é’Ÿ\n",
    "\n",
    "éšåç»§ç»­ä½¿ç”¨VADæ¨¡å‹,å°†éŸ³é¢‘åˆ‡æˆ10-20ç§’å·¦å³çš„çŸ­ç‰‡æ®µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    name = \"/\".join(x.split(\"/\")[:-1]).replace(\"/raw\",\"/wavs/\") + x.split(\"/\")[-1].split(\".\")[0]\n",
    "    return subprocess.run(f\"ffmpeg -hide_banner -loglevel panic -i '{x}' -ac 1 -ar 16000 -f s16le - \"+\n",
    "                   f\"| ffmpeg -hide_banner -loglevel panic -f s16le -ar 16000 -i - -f segment -segment_time {sg_len} {name}_%03d.wav\",\n",
    "                   shell=True)\n",
    "\n",
    "os.makedirs(os.path.join(start_path, \"wavs\"), exist_ok=True)\n",
    "wavs_name = glob(os.path.join(start_path, \"raw/*.*\"))\n",
    "\n",
    "# run(wavs_name[0]) # ç†è®ºä¸Šä½ å¯ä»¥çœ‹åˆ°,åœ¨wavsæ–‡ä»¶å¤¹ä¼šå‡ºç°å¾ˆå¤šçš„éŸ³é¢‘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè¿›ç¨‹è¿è¡Œ ffmpeg.\n",
    "res = pqdm(wavs_name, run, n_jobs=CPU_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3a723",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cff92",
   "metadata": {},
   "source": [
    "### VADæ•°æ®åˆ‡ç‰‡(é¢„è®¡30min)\n",
    "ç»§ç»­ä½¿ç”¨VADæ¨¡å‹,å°†éŸ³é¢‘åˆ‡æˆ10-20ç§’å·¦å³çš„çŸ­ç‰‡æ®µ\n",
    "\n",
    "`ps -ef | grep tmp/temp.py | grep -v grep | cut -c 9-16| xargs kill -9` æ¥killæ‰å·²ç»è·‘èµ·æ¥çš„è¿›ç¨‹.\n",
    "\n",
    "`watch -n1 \"cat vad/vad_*/1best_recog/text |wc -l\"` æ¥æŸ¥çœ‹æ¡æ•°.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda_devices = [0]*2\n",
    "cuda_devices = [0,1,2,3]*2 # æ˜¾å¡ä¸ªæ•°,å¤šå¡ä»£è¡¨0,1,2,3. å¦‚æœæ˜¯å•å¡,è®¾ç½®ä¸º [0]*2\n",
    "NJOB = len(cuda_devices) # ä½¿ç”¨å¤šå°‘ä¸ªè¿›ç¨‹åŒæ—¶è¿›è¡ŒVADè®¡ç®—. é€šå¸¸ä¸€ä¸ªGPUå¯ä»¥æ”¾ä¸‹2-4ä¸ªè¿›ç¨‹\n",
    "\n",
    "min_length = 2.0 # éŸ³é¢‘æœ€çŸ­é•¿åº¦, å¦‚æœå°äºè¯¥é•¿åº¦,ä¼šå’Œåé¢çš„ç»“æœåˆå¹¶\n",
    "max_length = 15.0 # éŸ³é¢‘æœ€é•¿é•¿åº¦, å¦‚æœè¶…è¿‡è¯¥é•¿åº¦, ä¼šç›´æ¥ä»ä¸­é—´ç å¼€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a936d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(start_path, \"tmp\")}\n",
    "os.makedirs(os.path.join(start_path, \"tmp\"), exist_ok=True)\n",
    "\n",
    "_script = f'''\n",
    "import os\n",
    "import sys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "start_path = '{start_path}'\n",
    "inference_pipeline = pipeline(\n",
    "    task=Tasks.voice_activity_detection,\n",
    "    model='damo/speech_fsmn_vad_zh-cn-16k-common-pytorch',\n",
    "    model_revision='v1.1.8',\n",
    "    output_dir=os.path.join(start_path, \"vad\", \"vad_\"+sys.argv[1]),\n",
    ")\n",
    "\n",
    "print(os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".scp\"))\n",
    "inference_pipeline(audio_in=os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".scp\"))\n",
    "'''\n",
    "with open(os.path.join(start_path, \"tmp\", \"temp.py\"), \"w\") as f:\n",
    "    f.write(_script)\n",
    "    \n",
    "data = glob(os.path.join(start_path,\"wavs\", \"*.wav\"))\n",
    "slice_len = (len(data) + NJOB - 1) // NJOB\n",
    "for IJOB in range(NJOB):\n",
    "    data_dir = data[IJOB*slice_len : (IJOB+1)*slice_len]\n",
    "    with open(os.path.join(start_path, \"tmp\", f\"temp_{IJOB}.scp\"), \"w\") as f:\n",
    "        for i in data_dir:\n",
    "            f.writelines(i.split(\"/\")[-1].split(\".\")[0] + \" \"+ i + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pqdm(list(zip(cuda_devices, list(range(NJOB)))), run_multiprocess, n_jobs=NJOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob(os.path.join(start_path,\"vad/*/1best_recog/*\"))\n",
    "os.makedirs(start_path + \"slices\", exist_ok=True)\n",
    "os.makedirs(start_path + \"metas\", exist_ok=True)\n",
    "\n",
    "with open(start_path+\"metas/meta.csv\", \"w\") as meta:\n",
    "    for data in tqdm(datas):\n",
    "        with open(data) as f:\n",
    "            f={i[:i.find(\" \")]:json.loads(i[i.find(\" \"):]) for i in f.readlines()}\n",
    "        for name in f:\n",
    "            wav, fs = sf.read(start_path + f\"wavs/{name}.wav\")\n",
    "            \n",
    "            if f[name] == []:\n",
    "                continue\n",
    "            \n",
    "            temp = []\n",
    "            count = 0\n",
    "            for idxs in f[name]:\n",
    "                temp.append([int(idxs[0] / 1000 * fs), int(idxs[1] / 1000 * fs)])\n",
    "                if (idxs[1] - idxs[0]) / 1000 < min_length:\n",
    "                    continue\n",
    "                \n",
    "                _wavs = [np.concatenate([wav[i[0]:i[1]] for i in temp])]\n",
    "                if (_wavs[0].shape[0] / fs) > max_length:\n",
    "                    _k = 2 ** np.ceil(np.log2((_wavs[0].shape[0]) / fs / max_length))\n",
    "                    for i in range(int(_k)):\n",
    "                        _wavs.append(_wavs[0][int(i/_k*_wavs[0].shape[0]) : int((i+1)/_k*_wavs[0].shape[0])])\n",
    "                    _wavs = _wavs[1:]\n",
    "                    \n",
    "                for _wav in _wavs:\n",
    "                    sf.write(start_path + f\"slices/{name}_{str(count).zfill(4)}.wav\", _wav, fs)\n",
    "                    _meta = {'audio_filepath': start_path + f\"slices/{name}_{str(count).zfill(4)}.wav\", \"duration\": round(_wav.shape[0]/fs, 3)}\n",
    "                    meta.writelines(json.dumps(_meta) + '\\n')\n",
    "                \n",
    "                    count += 1\n",
    "                temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc215d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(start_path+\"metas/meta.csv\") as meta:\n",
    "    meta = [json.loads(i) for i in meta.readlines()]\n",
    "    \n",
    "meta = pd.DataFrame(meta)\n",
    "print(\n",
    "    '15åˆ†ä½æ•°æ—¶é•¿', meta['duration'].quantile(0.15), \n",
    "    '\\n85åˆ†ä½æ•°æ—¶é•¿', meta['duration'].quantile(0.85), \n",
    "    '\\næ€»æ—¶é•¿', round(meta['duration'].sum()/60/60, 5)\n",
    "     )\n",
    "\n",
    "print(meta.head(5))\n",
    "Audio(random.choice(meta.values)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa665e0",
   "metadata": {},
   "source": [
    "-----\n",
    "### modelscope ASRè¯†åˆ«(é¢„è®¡30min)\n",
    "\n",
    "ç›´æ¥è°ƒç”¨modelscopeçš„ASRæ¨¡å‹.\n",
    "\n",
    "`watch -n1 \"cat asr/asr_*/1best_recog/text |wc -l\"` æ¥çœ‹å½“å‰è¿›åº¦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b505f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_devices = [0,1,2,3] * 2\n",
    "NJOB = len(cuda_devices) \n",
    "\n",
    "# batchå–å†³äºä½ çš„æ˜¾å¡. æ˜¾å­˜å¤šå¯ä»¥é€‚å½“æ‹‰å¤§\n",
    "# æ³¨æ„ä¸€ä¸‹, batchå˜å¤§ä¼¼ä¹ä¼šä½¿ç»“æœæœ‰è½»å¾®ä¸‹é™.\n",
    "# åŒæ—¶,batchå¤§äº1çš„æ—¶å€™,ä¼šå‡ºç°ä¹‹å‰æåˆ°çš„ `@` çš„é—®é¢˜.\n",
    "# ä½†æ˜¯å¾ˆéšæœº,å¯èƒ½å°±å‡ æ¡éŸ³é¢‘ä¼šå‡ºç°. å¦‚æœä¸æƒ³æ”¹ä»£ç ,å°±1.\n",
    "BATCH = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(start_path, \"tmp\")}\n",
    "os.makedirs(os.path.join(start_path, \"tmp\"), exist_ok=True)\n",
    "\n",
    "_script = f'''\n",
    "import os\n",
    "import sys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "start_path = '{start_path}'\n",
    "inference_pipeline = pipeline(\n",
    "    task=Tasks.auto_speech_recognition,\n",
    "    model='damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch',\n",
    "    output_dir=os.path.join(start_path, \"asr\", \"asr_\"+sys.argv[1]),\n",
    "    batch={BATCH},\n",
    ")\n",
    "\n",
    "print(os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".scp\"))\n",
    "inference_pipeline(audio_in=os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".scp\"))\n",
    "'''\n",
    "\n",
    "with open(os.path.join(start_path, \"tmp\", \"temp.py\"), \"w\") as f:\n",
    "    f.write(_script)\n",
    "    \n",
    "data = glob(os.path.join(start_path, \"slices\", \"*.wav\"))\n",
    "slice_len = (len(data) + NJOB - 1) // NJOB\n",
    "for IJOB in range(NJOB):\n",
    "    data_dir = data[IJOB*slice_len : (IJOB+1)*slice_len]\n",
    "    with open(os.path.join(start_path, \"tmp\", f\"temp_{IJOB}.scp\"), \"w\") as f:\n",
    "        for i in data_dir:\n",
    "            f.writelines(i.split(\"/\")[-1].split(\".\")[0] + \" \"+ i + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5350a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pqdm(list(zip(cuda_devices, list(range(NJOB)))), run_multiprocess, n_jobs=NJOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2171ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob(os.path.join(start_path,\"asr/*/1best_recog/text\"))\n",
    "\n",
    "try:\n",
    "    with open(start_path+\"metas/meta.csv\") as meta:\n",
    "        meta = [json.loads(i) for i in meta.readlines()]\n",
    "    meta = pd.DataFrame(meta).set_index('audio_filepath')\n",
    "except:\n",
    "    meta = pd.read_csv(start_path + \"metas/meta.csv\").set_index(\"audio_filepath\")\n",
    "meta['_text'] = None\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    with open(data) as f:\n",
    "        _f = {}\n",
    "        for i in f.readlines():\n",
    "            if len(i.strip().split()) > 1:\n",
    "                _f[start_path +'slices/'+ i.split()[0] + '.wav'] = \" \".join(i.split()[1:]).strip()\n",
    "    for name in _f:\n",
    "        meta.at[name, '_text'] = _f[name]\n",
    "        \n",
    "data_dir = meta[meta['_text'].apply(lambda x: x is None)].reset_index()['audio_filepath'].values.tolist()\n",
    "if len(data_dir) > 0:        \n",
    "    print(\"æœ‰å¥å­æ²¡æå‡ºæ¥? batchä¸º1çš„æ—¶å€™åº”è¯¥éƒ½èƒ½æå‡ºæ¥æ‰å¯¹.\")\n",
    "    \n",
    "meta = meta[meta['_text'].apply(lambda x: x is not None)].reset_index()\n",
    "meta.to_csv(start_path + \"metas/meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc32b6",
   "metadata": {},
   "source": [
    "\n",
    "### modelscope æ ‡ç‚¹ç¬¦å· (é¢„è®¡10min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(start_path, \"tmp\")}\n",
    "os.makedirs(os.path.join(start_path, \"tmp\"), exist_ok=True)\n",
    "\n",
    "_script = f'''\n",
    "import os\n",
    "import sys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "start_path = '{start_path}'\n",
    "inference_pipeline = pipeline(\n",
    "    task=Tasks.punctuation,\n",
    "    model='damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch',\n",
    "    output_dir=os.path.join(start_path, \"punc\", \"punc_\"+sys.argv[1])\n",
    ")\n",
    "\n",
    "print(os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".txt\"))\n",
    "rec_result = inference_pipeline(text_in=os.path.join(start_path, 'tmp', \"temp_\"+sys.argv[1]+\".txt\"))    \n",
    "'''\n",
    "with open(os.path.join(start_path, \"tmp\", \"temp.py\"), \"w\") as f:\n",
    "    f.write(_script)\n",
    "    \n",
    "meta = pd.read_csv(start_path + \"metas/meta.csv\")\n",
    "slice_len = (len(meta) + NJOB - 1) // NJOB\n",
    "for IJOB in range(NJOB):\n",
    "    data_dir = meta.values[IJOB*slice_len : (IJOB+1)*slice_len]\n",
    "    with open(os.path.join(start_path, \"tmp\", f\"temp_{IJOB}.txt\"), \"w\") as f:\n",
    "        for i in data_dir:\n",
    "            f.writelines(i[0].split(\"/\")[-1].split(\".\")[0] + \"\\t\"+ i[2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pqdm(list(zip(cuda_devices, list(range(NJOB)))), run_multiprocess, n_jobs=NJOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin, Style\n",
    "def get_pinyin(text):\n",
    "    text = text.lower()\n",
    "    initials = lazy_pinyin(text, neutral_tone_with_five=False, style=Style.INITIALS, strict=False)\n",
    "    finals = lazy_pinyin(text, neutral_tone_with_five=False, style=Style.FINALS_TONE3)\n",
    "\n",
    "    text_phone = []\n",
    "    for _o in zip(initials, finals):\n",
    "        if _o[0] != _o[1] and _o[0] != '':\n",
    "            _o = ['@'+i for i in _o]\n",
    "            text_phone.extend(_o)\n",
    "        elif _o[0] != _o[1] and _o[0] == '':\n",
    "            text_phone.append('@'+_o[1])\n",
    "        else:\n",
    "            text_phone.extend(list(_o[0]))\n",
    "\n",
    "    text_phone = \" \".join(text_phone)\n",
    "    return text_phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob(os.path.join(start_path,\"punc/*/infer.out\"))\n",
    "meta = pd.read_csv(start_path + \"metas/meta.csv\").set_index(\"audio_filepath\")\n",
    "\n",
    "meta['text'] = None\n",
    "meta['_text_with_punc'] = None\n",
    "\n",
    "for data in tqdm(datas):\n",
    "    with open(data) as f:\n",
    "        f = {start_path +'slices/'+ i.split(\"\\t\")[0] + '.wav': i.split(\"\\t\")[-1].strip() for i in f.readlines()}\n",
    "        \n",
    "    for name in f:\n",
    "        text = \"\".join(f[name].split())\n",
    "        meta.at[name, 'text'] = text\n",
    "        meta.at[name, '_text_with_punc'] = get_pinyin(text)\n",
    "            \n",
    "meta = meta.reset_index()\n",
    "meta.to_csv(start_path + \"metas/meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­puncå’ŒNone, å‡ºç°è¿‡å¥‡å¥‡æ€ªæ€ªçš„bug\n",
    "# å‡ºç°äº†è¯·è”ç³»æˆ‘ğŸ˜‚.\n",
    "\n",
    "def _if_continue_punc(x):\n",
    "    punc = False\n",
    "    for i in list(x):\n",
    "        if i in 'ã€ï¼Œã€‚ï¼Ÿï¼ï½â€¦â€”':\n",
    "            if punc == True:\n",
    "                return True\n",
    "            else:\n",
    "                punc = True\n",
    "        else:\n",
    "            punc = False\n",
    "    return False\n",
    "        \n",
    "meta[meta['text'].apply(lambda x:_if_continue_punc(x) or x is None or x == '')] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee5d07",
   "metadata": {},
   "source": [
    "### è½¬åˆ°ç›®æ ‡SR\n",
    "ç”±äºåœ¨modelscopeçš„è¿ç®—æ—¶,ä¼šè°ƒç”¨é™é‡‡æ ·ç®—æ³•åˆ°16000,é€Ÿåº¦å¾ˆæ…¢,æ‰€ä»¥ä¸€å¼€å§‹å…ˆç»Ÿä¸€åˆ°äº†16000.\n",
    "\n",
    "ä¸ºäº†åœ¨ç›®æ ‡é‡‡æ ·ç‡ä¸Šè®­ç»ƒ,éœ€è¦é‡æ–°è¿›è¡Œç¬¬ä¸€æ­¥çš„æ“ä½œ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb953f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    name = \"/\".join(x.split(\"/\")[:-1]).replace(\"/raw\",\"/wavs/\") + x.split(\"/\")[-1].split(\".\")[0]\n",
    "    return subprocess.run(f\"ffmpeg -hide_banner -loglevel panic -i '{x}' -ac 1 -ar {SR} -f s16le - \"+\n",
    "                   f\"| ffmpeg -hide_banner -loglevel panic -f s16le -ar {SR} -i - -f segment -segment_time {sg_len} {name}_%03d.wav\",\n",
    "                   shell=True)\n",
    "\n",
    "os.makedirs(os.path.join(start_path, \"wavs\"), exist_ok=True)\n",
    "wavs_name = glob(os.path.join(start_path, \"raw/*.*\"))\n",
    "\n",
    "res = pqdm(wavs_name, run, n_jobs=CPU_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447aaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob(os.path.join(start_path,\"vad/*/1best_recog/*\"))\n",
    "for data in tqdm(datas):\n",
    "    with open(data) as f:\n",
    "        f={i[:i.find(\" \")]:json.loads(i[i.find(\" \"):]) for i in f.readlines()}\n",
    "    for name in f:\n",
    "        wav, fs = sf.read(start_path + f\"wavs/{name}.wav\")\n",
    "        assert fs == SR\n",
    "        \n",
    "        if f[name] == []:\n",
    "            continue\n",
    "\n",
    "        temp = []\n",
    "        count = 0\n",
    "        for idxs in f[name]:\n",
    "            temp.append([int(idxs[0] / 1000 * fs), int(idxs[1] / 1000 * fs)])\n",
    "            if (idxs[1] - idxs[0]) / 1000 < min_length:\n",
    "                continue\n",
    "\n",
    "            _wavs = [np.concatenate([wav[i[0]:i[1]] for i in temp])]\n",
    "            if (_wavs[0].shape[0] / fs) > max_length:\n",
    "                _k = 2 ** np.ceil(np.log2((_wavs[0].shape[0]) / fs / max_length))\n",
    "                for i in range(int(_k)):\n",
    "                    _wavs.append(_wavs[0][int(i/_k*_wavs[0].shape[0]) : int((i+1)/_k*_wavs[0].shape[0])])\n",
    "                _wavs = _wavs[1:]\n",
    "\n",
    "            for _wav in _wavs:\n",
    "                sf.write(start_path + f\"slices/{name}_{str(count).zfill(4)}.wav\", _wav, fs)\n",
    "                count += 1\n",
    "            temp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495083",
   "metadata": {},
   "source": [
    "-----\n",
    "## è·å–è¾…åŠ©ç‰¹å¾\n",
    "\n",
    "### è·å–bertç‰¹å¾ (é¢„è®¡30min)\n",
    "å°†bertå¾—åˆ°çš„feat,æŒ‰ç…§éŸ³ç´ çš„ç»“æ„è¿›è¡Œç®€å•çš„å¤åˆ¶.\n",
    "\n",
    "æ¯”å¦‚ æˆ‘çš„A(1 2 3): w o3 d e4 a -> æˆ‘ æˆ‘ çš„ çš„ a -> 1 1 2 2 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940981bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpt = False # ä½¿ç”¨GPT2ç‰¹å¾æˆ–Bert-basedç‰¹å¾,å¥½åƒå¤§å·®ä¸å·®."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {os.path.join(start_path, \"tmp\")}\n",
    "os.makedirs(os.path.join(start_path, \"tmp\"), exist_ok=True)\n",
    "\n",
    "_script = f'''\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pypinyin import lazy_pinyin, Style\n",
    "\n",
    "if {not use_gpt}:\n",
    "    from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "\n",
    "if {use_gpt}:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "\n",
    "start_path = '{start_path}'\n",
    "\n",
    "NJOB={NJOB}\n",
    "meta = pd.read_csv(start_path + \"metas/meta.csv\")\n",
    "slice_len = (len(meta) + NJOB - 1) // NJOB\n",
    "meta = meta.loc[int(sys.argv[1])*slice_len : (int(sys.argv[1])+1)*slice_len]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "_script += f'''\n",
    "os.makedirs(start_path + 'bert_feats/', exist_ok=True)\n",
    "for series in tqdm(meta.iloc):\n",
    "    name = start_path + \"bert_feats/\" + series['audio_filepath'].split(\"/\")[-1].replace(\".wav\",\".npy\")\n",
    "    text = series['text']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        for i in inputs:\n",
    "            inputs[i] = inputs[i].to(device)\n",
    "        res = model(**inputs, output_hidden_states=True)\n",
    "        res = torch.cat(res['hidden_states'][-3:-2], -1)[0].cpu().numpy() # æœ‰soså’Œeos token\n",
    "    \n",
    "    initials = lazy_pinyin(text, neutral_tone_with_five=False, style=Style.INITIALS, strict=False)\n",
    "    finals = lazy_pinyin(text, neutral_tone_with_five=False, style=Style.FINALS_TONE3)\n",
    "    \n",
    "    _vecs = []\n",
    "    _text = []\n",
    "    _chars = []\n",
    "    for _o in zip(zip(initials, finals), text, res[1:-1]):\n",
    "        _o, _c, _vec = _o\n",
    "        if _o[0] != _o[1] and _o[0] != '':\n",
    "            _text.extend(['@'+i for i in _o])\n",
    "            _chars.extend([_c]*2)\n",
    "            _vecs.extend([_vec]*2)\n",
    "        elif _o[0] != _o[1] and _o[0] == '':\n",
    "            _text.append('@'+_o[1])\n",
    "            _chars.append(_c)\n",
    "            _vecs.append(_vec)\n",
    "        else:\n",
    "            _text.extend(list(_o[0]))  \n",
    "            _chars.extend([_c]*len(_o[0]))\n",
    "            _vecs.extend([_vec]*len(_o[0]))\n",
    "    try:\n",
    "        assert len(_text) == len(_chars)\n",
    "        assert len(_vecs) == len(_text)\n",
    "    except:\n",
    "        print(name)\n",
    "        continue\n",
    "        \n",
    "    _vecs = np.stack([res[0]] + _vecs + [res[-1]])\n",
    "    np.save(name, _vecs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74aa5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(start_path, \"tmp\", \"temp.py\"), \"w\") as f:\n",
    "    f.write(_script)\n",
    "res = pqdm(list(zip(cuda_devices, list(range(NJOB)))), run_multiprocess, n_jobs=NJOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ed886",
   "metadata": {},
   "source": [
    "------\n",
    "## To NEMO format & Ready to go.\n",
    "å°†metaè½¬ä¸ºnemoéœ€è¦çš„æ ¼å¼. å¯ä»¥å¼€å§‹è¿è¡Œå•¦!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354188a",
   "metadata": {},
   "source": [
    "### æ‰“åŒ…æˆéœ€è¦çš„æ•°æ®æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ef5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_valid = 128 # validæ•°æ®é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_to_nemo(meta, g):\n",
    "    for series in tqdm(meta):\n",
    "        lines = {}\n",
    "        lines['audio_filepath'] = start_path + 'slices/' + series[0].split(\"/\")[-1]\n",
    "        lines['duration'] = series[1]\n",
    "        lines['text'] = series[-2]\n",
    "        \n",
    "        temp = []\n",
    "        for j in series[-1].split():\n",
    "            if True:\n",
    "                temp.append(j)\n",
    "        if len(temp) == 0:\n",
    "            print(i)\n",
    "            continue\n",
    "        lines['normalized_text'] = \" \".join(temp)\n",
    "        \n",
    "        g.writelines(json.dumps(lines, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(start_path + 'metas/meta.csv')\n",
    "\n",
    "portion = (meta['_text_with_punc'].apply(lambda x:len(x.split())) / meta['duration'])\n",
    "meta_processed = meta[(portion >= portion.quantile(0.05)) &(portion <= portion.quantile(0.95))]\n",
    "meta_processed = meta_processed[(3 < meta_processed['duration']) & (meta_processed['duration'] < 13)]\n",
    "meta_processed = meta_processed.values\n",
    "\n",
    "print(\"æ€»è®¡:\", len(meta_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d473a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(meta_processed)\n",
    "os.makedirs(start_path + \"metas/nemo\", exist_ok=True)\n",
    "\n",
    "with open(start_path + \"metas/nemo/train_manifest.json\", \"w\") as g:\n",
    "    meta_to_nemo(meta_processed[:-for_valid], g)\n",
    "        \n",
    "with open(start_path + \"metas/nemo/val_manifest.json\", \"w\") as g:\n",
    "    meta_to_nemo(meta_processed[-for_valid:], g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69767e66",
   "metadata": {},
   "source": [
    "### è®­ç»ƒFastpitch(é¢„è®¡7h)\n",
    "\n",
    "ä¸»è¦æ˜¯2æ­¥,é¦–å…ˆæ˜¯è®­ç»ƒfastpitch,æ³¨æ„è¦å®‰è£…ä¸€ä¸‹NeMoçš„requirement.\n",
    "\n",
    "10å°æ—¶æ•°æ®æ¯epochçº¦50ç§’,é¢„è®¡è®­ç»ƒ500epoch,7å°æ—¶åå¯ä»¥è®­ç»ƒå®Œæ¯•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ada8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(start_path, \"codes\"), exist_ok=True)\n",
    "!cd {os.path.join(start_path, \"codes\")} && git clone https://github.com/NVIDIA/NeMo.git    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14b23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ä¿®æ”¹ä¸€äº›æ–‡ä»¶\n",
    "##################################\n",
    "# 1. æ•°æ®ç±»çš„ä¸»è¦æ˜¯å¢åŠ äº†ä¸€ä¸ªtokenizer. æ–¹ä¾¿äºæµ‹è¯•MoeGoeçš„å›½é™…éŸ³æ ‡(å°½ç®¡å¤šè¯­ä¼šæœ‰å¸®åŠ©, ä¸­æ–‡å¹¶æ²¡æœ‰ä»€ä¹ˆå®é™…çš„ä½œç”¨, æ‰€ä»¥å®é™…è¿˜æ˜¯ç”¨çš„pinyin)\n",
    "# 2. å¢åŠ äº†bertç‰¹å¾, éœ€è¦æ”¹åŠ¨æ¨¡å‹ä»£ç ä»¥åŠload dataçš„ä»£ç .\n",
    "# 3. ç”±äºä½¿ç”¨äº†pyinä½œä¸ºpitchçš„æå–å™¨,é¦–ä¸ªepoché€Ÿåº¦ä¼šå¾ˆæ…¢,æ”¹æˆäº†pyworldé€Ÿåº¦èƒ½å¿«ä¸å°‘\n",
    "# 4. ä¸€äº›åŸºæœ¬çš„è°ƒå‚å‚æ•°æ”¹äº†ä¸€ä¸‹, å¦‚å­¦ä¹ ç‡, batchsizeç­‰ç­‰\n",
    "# 5. nemoç‰ˆçš„MASä½¿ç”¨çš„æ˜¯numba, é»˜è®¤å æ»¡æ‰€æœ‰çš„CPUèµ„æº, å®é™…ä¸Šé™åˆ¶threadåˆ°4é€Ÿåº¦ä¸ä¼šå—åˆ°å½±å“.\n",
    "# 6. æœ€å,ä½¿ç”¨fp16æ¥è®­. å¯¹äºä¸€äº›å¾ˆç³Ÿç³•çš„æ•°æ®æœ‰å¯èƒ½æŠŠä»–trainçˆ†, çˆ†äº†æ¢32è·‘(ä½†å…¶å®æ¦‚ç‡éå¸¸ä½). \n",
    "##################################\n",
    "\n",
    "!cp ./replace_files/tts_tokenizers.py {start_path}/codes/NeMo/nemo/collections/common/tokenizers/text_to_speech/tts_tokenizers.py\n",
    "!cp ./replace_files/dataset.py {start_path}/codes/NeMo/nemo/collections/tts/data/dataset.py\n",
    "!cp ./replace_files/tts_data_types.py {start_path}/codes/NeMo/nemo/collections/tts/torch/tts_data_types.py\n",
    "\n",
    "!cp ./replace_files/fs_model.py {start_path}/codes/NeMo/nemo/collections/tts/models/fastpitch.py\n",
    "!cp ./replace_files/fs_modules.py {start_path}/codes/NeMo/nemo/collections/tts/modules/fastpitch.py\n",
    "!cp ./replace_files/bert_tsfm.py {start_path}/codes/NeMo/nemo/collections/tts/modules/transformer.py\n",
    "\n",
    "!cp ./replace_files/fastpitch_align_v1.05.yaml {start_path}/codes/NeMo/examples/tts/conf/fastpitch_align_v1.05.yaml\n",
    "!cp ./replace_files/fs_run.py {start_path}/codes/NeMo/examples/tts/fastpitch.py\n",
    "\n",
    "# !cd {start_path}/codes/NeMo && ./reinstall.sh\n",
    "!cd {start_path}/codes/NeMo && cat ./requirements/requirements_tts.txt ./requirements/requirements_asr.txt \\\n",
    "./requirements/requirements_common.txt ./requirements/requirements_lightning.txt ./requirements/requirements.txt > ./requirements.txt \\\n",
    "&& pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–ä¸€ä¸‹æ‰€éœ€è¦çš„pitch,éšåç”¨äºè®¡ç®—pitchçš„å€¼\n",
    "# echoå¾—åˆ°çš„å€¼è¿è¡Œä¸€ä¸‹, å¦‚æœåœ¨colabä¸Šçš„è¯å¯ä»¥åˆ é™¤echoç›´æ¥è¿è¡Œ.\n",
    "\n",
    "!echo PYTHONPATH={start_path}/codes/NeMo CUDA_VISIBLE_DEVICES=0 python {start_path}/codes/NeMo/examples/tts/fastpitch.py \\\n",
    "    train_dataset={start_path}/metas/nemo/train_manifest.json \\\n",
    "    validation_datasets={start_path}/metas/nemo/val_manifest.json \\\n",
    "    sup_data_path={start_path}/sup_data \\\n",
    "    exp_manager.exp_dir={start_path}/tmp \\\n",
    "    bert_path={start_path}/bert_feats \\\n",
    "    trainer.strategy=null name=testing pitch_mean=130.01991 pitch_std=50.18665 trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8461f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–pitchçš„å‡å€¼æ–¹å·®. é¦–å…ˆé€šè¿‡nemoè‡ªå·±ç”Ÿæˆsupæ–‡ä»¶,stopåé‡æ–°è®¡ç®—.\n",
    "\n",
    "temp = []\n",
    "for i in tqdm(glob(start_path + \"sup_data/pitch/*\")):\n",
    "    _pitch = torch.load(i)\n",
    "    _pitch = _pitch[_pitch!=0].numpy()\n",
    "    temp.append(_pitch)\n",
    "    \n",
    "_pitch = np.concatenate(temp)\n",
    "pmax, pmin = _pitch.max(), _pitch.min()\n",
    "pstd, pmean = _pitch.std(), _pitch.mean()\n",
    "\n",
    "print(pmax, pmin)\n",
    "print(pstd, pmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148df702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ!\n",
    "\n",
    "!echo PYTHONPATH={start_path}/codes/NeMo CUDA_VISIBLE_DEVICES=0 python {start_path}/codes/NeMo/examples/tts/fastpitch.py \\\n",
    "    train_dataset={start_path}/metas/nemo/train_manifest.json \\\n",
    "    validation_datasets={start_path}/metas/nemo/val_manifest.json \\\n",
    "    sup_data_path={start_path}/sup_data \\\n",
    "    exp_manager.exp_dir={start_path}/results \\\n",
    "    bert_path={start_path}/bert_feats \\\n",
    "    trainer.strategy=null name=fs2 pitch_mean={pmean} pitch_std={pstd} pitch_fmin={pmin} pitch_fmax={pmax} \n",
    "#     +init_from_pretrained_model='tts_zh_fastpitch_sfspeech'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6074e50",
   "metadata": {},
   "source": [
    "### è®­ç»ƒHIFIGAN(é¢„è®¡?)\n",
    "\n",
    "è¯¥æ­¥æ—¶é—´è¾ƒé•¿,ä¸€èˆ¬trainçš„æ—¶é—´è¶Šé•¿è¶Šä¸fuzzy.\n",
    "\n",
    "ç­‰åˆ°å‡ºäº†ckptå°±å¯ä»¥å®šæœŸåœ¨ä¸‹é¢çš„inferé‚£é‡Œcheckä¸€ä¸‹.\n",
    "\n",
    "hifigançš„è®­ç»ƒæ˜¯æ¯”è¾ƒæ…¢çš„, å› ä¸ºæ²¡æœ‰FP16(FP16ä¼šæœ‰ä¸€å®šçš„ä¸‹é™)\n",
    "<br>ä¸€ä¸ªepochçº¦3åˆ†é’Ÿ,é€šå¸¸è¦100ä¸ªepochå·¦å³å³5ä¸ªå°æ—¶."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c25b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–è®­ç»ƒfs2çš„melçš„ç»“æœ!\n",
    "\n",
    "!echo PYTHONPATH={start_path}/codes/NeMo CUDA_VISIBLE_DEVICES=0 python {start_path}/codes/NeMo/examples/tts/fastpitch.py \\\n",
    "    train_dataset={start_path}/metas/nemo/train_manifest.json \\\n",
    "    validation_datasets={start_path}/metas/nemo/val_manifest.json \\\n",
    "    sup_data_path={start_path}/sup_data \\\n",
    "    exp_manager.exp_dir={start_path}/results \\\n",
    "    bert_path={start_path}/bert_feats \\\n",
    "    name=fs2 pitch_mean={pmean} pitch_std={pstd} pitch_fmin={pmin} pitch_fmax={pmax} \\\n",
    "    model.train_ds.dataloader_params.batch_size=1 trainer.precision=32 \\\n",
    "    model.get_mel_result={start_path}/sup_data/pred_mels \\\n",
    "    trainer.strategy=null trainer.max_epochs=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆmetaæ–‡ä»¶\n",
    "\n",
    "with open(f'{start_path}/metas/nemo/train_manifest_mel.json', \"w\") as g:\n",
    "    for s in tqdm(glob(f'{start_path}/sup_data/pred_mels/*.wav')):\n",
    "        _wav, _fs = sf.read(s)\n",
    "        _dict = {\n",
    "            'audio_filepath': s,\n",
    "            'duration': round(_wav.shape[0] / _fs, 3),\n",
    "            'mel_filepath': s.replace(\".wav\", \".npy\"),\n",
    "        }\n",
    "        g.writelines(json.dumps(_dict, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "!head -n32 {start_path}/metas/nemo/train_manifest_mel.json > {start_path}/metas/nemo/val_manifest_mel.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune HIFIGAN.\n",
    "# è¿™ä¸€æ­¥ä¼šä¸‹è½½nemoçš„ckpt, å¯èƒ½ä¼šå¾ˆæ…¢>_<\n",
    "\n",
    "!echo PYTHONPATH={start_path}/codes/NeMo CUDA_VISIBLE_DEVICES=0 python {start_path}/codes/NeMo/examples/tts/hifigan_finetune.py \\\n",
    "    train_dataset={start_path}/metas/nemo/train_manifest_mel.json \\\n",
    "    validation_datasets={start_path}/metas/nemo/val_manifest_mel.json \\\n",
    "    exp_manager.exp_dir={start_path}/results \\\n",
    "    model/train_ds=train_ds_finetune model/validation_ds=val_ds_finetune \\\n",
    "    trainer.strategy=null name=hifigan \\\n",
    "    +init_from_pretrained_model='tts_zh_hifigan_sfspeech' \\\n",
    "    --config-name hifigan.yaml trainer.check_val_every_n_epoch=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf5f2a",
   "metadata": {},
   "source": [
    "----------\n",
    "## Infer with NeMo\n",
    "\n",
    "å¦‚æœä¸Šè¿°ä»»åŠ¡å‡è®­ç»ƒå®Œæ¯•, å¯ä»¥å¼€å§‹è¿›è¡Œinfer.\n",
    "\n",
    "æŒ‰é¡ºåºæ‰§è¡Œä»£ç å³å¯,ä¿®æ”¹raw_textä¸ºä½ éœ€è¦çš„æ–‡æœ¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ede162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.disable(logging.ERROR)\n",
    "sys.path.append(start_path + \"codes/NeMo\")\n",
    "from nemo.collections.tts.models import HifiGanModel, FastPitchModel\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device='cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "bert_model = bert_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa869775",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hifigan\n",
    "hfg_path = glob(start_path + \"results/hifigan/*/checkpoints/*last*\")\n",
    "if len(hfg_path) > 0:\n",
    "    vocoder_model_pt = HifiGanModel.load_from_checkpoint(checkpoint_path=hfg_path[0]).eval().to(device)\n",
    "else:\n",
    "    # å¦‚æœæ²¡trainå®Œå¯ä»¥å¬ä¸ªåŠ¨é™\n",
    "    print(\"ä½¿ç”¨nemoç»™çš„voocder,æƒ³è¾¾åˆ°æ›´å¥½æ•ˆæœéœ€è¦tune!\")\n",
    "    vocoder_model_pt = HifiGanModel.from_pretrained(model_name=\"tts_zh_hifigan_sfspeech\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fastpitch\n",
    "fastpitch_model_path =glob(start_path + \"results/fs2/checkpoints/*last*\")[0]\n",
    "spec_gen_model = FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_model_path).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨è¿™é‡Œä¿®æ”¹.\n",
    "raw_text = \"å¤§å®¶å¥½æˆ‘æ˜¯äºŒæ¬¡å…ƒå³°å“¥\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_e2c = {',':'ï¼Œ', '.':'ã€‚','?':'ï¼Ÿ',\"ã€\":\"ï¼Œ\"}\n",
    "raw_text=\"\".join([punc_e2c[i] if i in punc_e2c else i for i in raw_text])\n",
    "text = raw_text.replace(\" \",\"\").lower()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(\"\".join(text), return_tensors='pt')\n",
    "    for i in inputs:\n",
    "        inputs[i] = inputs[i].to(device)\n",
    "    res = bert_model(**inputs, output_hidden_states=True)\n",
    "    res = torch.cat(res['hidden_states'][-3:-2], -1)[0].cpu().numpy()\n",
    "\n",
    "initials = lazy_pinyin(raw_text, neutral_tone_with_five=False, style=Style.INITIALS, strict=False)\n",
    "finals = lazy_pinyin(raw_text, neutral_tone_with_five=False, style=Style.FINALS_TONE3)\n",
    "\n",
    "_vecs = []\n",
    "_text = []\n",
    "for _o in zip(zip(initials, finals), list(raw_text), res[1:-1]):\n",
    "    _o, _c, _vec = _o\n",
    "    if _o[0] != _o[1] and _o[0] != '':\n",
    "        _text.extend(['@'+i for i in _o])\n",
    "        _vecs.extend([_vec]*2)\n",
    "    elif _o[0] != _o[1] and _o[0] == '':\n",
    "        _text.append('@'+_o[1])\n",
    "        _vecs.append(_vec)\n",
    "    else:\n",
    "        _text.extend(list(_o[0]))\n",
    "        _vecs.extend([_vec]*len(_o[0]))\n",
    "\n",
    "_vecs = np.stack([res[0]] + _vecs + [res[-1]])\n",
    "bert = torch.tensor(_vecs.T[None]).to(device)\n",
    "phoneme = \" \".join(_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f99e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    parsed = spec_gen_model.parse(str_input=phoneme, normalize=False)\n",
    "    res = spec_gen_model.generate_spectrogram(\n",
    "        tokens=parsed, pace=1,\n",
    "        bert_feats=bert,\n",
    "    )\n",
    "    spectrogram = res\n",
    "    audio = vocoder_model_pt.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "audio = audio.to('cpu').numpy()\n",
    "# audio = audio / np.abs(audio).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31708c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iply.display(iply.Audio(audio[0], rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28dca6",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "1. å¯¹äºTTSä»»åŠ¡è€Œè¨€, é€šå¸¸10ä¸ªå°æ—¶å°±è¶³ä»¥å¾—åˆ°å¥½çš„æ•ˆæœ. å¦‚æœæœ‰ä¸€ä¸ªå¥½çš„åº•æ¨¡,è®­ç»ƒé€Ÿåº¦èƒ½æœ‰æ›´å¤§çš„æå‡.\n",
    "2. å¯¹äºå¤§å¤šæ•°æƒ…å†µ, ä¸éœ€è¦è®­ç»ƒVITS, å¯¹äºè§†é¢‘åˆ¶ä½œ,ç›´æ’­è€Œè¨€, fs2å°±å¤Ÿäº†. \n",
    "3. æ›´å¤šçš„æ•°æ®æ„å‘³æ›´å¥½å¾—æ•ˆæœ, ä¸è®ºæ˜¯chatgptè¿˜æ˜¯stable diff, éƒ½å‘Šè¯‰æˆ‘ä»¬æ•°æ®å±‚é¢çš„å†›å¤‡ç«èµ›è¿œæ²¡æœ‰ç»“æŸ. åœ¨ä½œè€…è‡ªå·±çš„å®éªŒä¸Šæ¥çœ‹,æ•°æ®ä»10håˆ°50h,æ•ˆæœæ˜¯ä¸€ç›´åœ¨æé«˜çš„.\n",
    "3. æ›´å¥½çš„æ¨¡å‹æ„å‘³æ›´å¤§çš„é£é™©, æ¯”å¦‚ç”¨chatgptè¯ˆéª—, unstable diffç”Ÿæ¶©å›¾éƒ½æ˜¯æŠ€æœ¯çš„åé¢. é¿å…æŠ€æœ¯çš„åé¢å°†ä¸€ç›´æ˜¯é‡è¦çš„æŒ‘æˆ˜. åœ¨åˆ¶ä½œè§†é¢‘æœŸé—´,ç½‘ä¿¡åŠä¹Ÿèµ·è‰äº†æ–°çš„ç®¡ç†åŠæ³•,ç›¸ä¿¡åœ¨æ³•å¾‹çš„è§„èŒƒä¸‹,æŠ€æœ¯ä¹Ÿèƒ½é€æ¸æ›´å¥½çš„æœåŠ¡æ¯ä¸€ä¸ªäºº.\n",
    "\n",
    "æœ€å,åˆ¶ä½œè§†é¢‘å’Œnotebookéƒ½èŠ±è´¹äº†ä¸å°‘æ—¶é—´å’Œç²¾åŠ›,å¸Œæœ›å¤§å®¶å¤šå¤šå…³æ³¨, å¤šå¤šä¸‰è¿.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
